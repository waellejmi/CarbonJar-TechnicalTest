{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01ca277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from typing import Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c526dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "regulatory_text = \"\"\"\n",
    "Warda Bidtha Inc. must report its Scope 1 emissions by 31 March , 2026.\n",
    "The limit for Sector B in Tunisia is 50,000 tCO2e.\n",
    "Green Energy Solutions Ltd. shall submit quarterly reports to the Environmental Protection Agency by the 15th of each quarter.\n",
    "Maximum allowable emissions for manufacturing facilities is 25,000 tons CO2 equivalent per year.\n",
    "LEONI received a penalty of $2.5 million for exceeding emission limits in Germany.\n",
    "All companies in the automotive sector must comply with Euro 6 standards before 1 January, 2026.\n",
    "AGIL Petroleum plc reported 45,200 tCO2e in their 2024 sustainability report.\n",
    "The deadline for carbon offset submissions is 31 December, 2025.\n",
    "Renewable energy targets for utilities: 30% by 2030, 50% by 2035.\n",
    "Mohamed Aziz and his company Wicrosoft are responsible for reducing emissions by 20% by 2027. because before in 2023 they emitted 100,000 tCO2e and they got fined for $300,000.\n",
    "Everyone must comply with CBAM regulations in Algeria by 1 September, 2025.\n",
    "The industry F must reduce its methane emissions by 30% by 2030.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d9d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "class RegulatoryNERPipeline:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "            self.ner_pipeline = pipeline(   \n",
    "                \"ner\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                aggregation_strategy=\"simple\"\n",
    "            )\n",
    "            self.model_name = \"dslim/bert-base-NER\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing NER pipeline: {e}\")\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Dict]:\n",
    "        try:\n",
    "            entities = self.ner_pipeline(text)\n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "ner_extractor = RegulatoryNERPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a8be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplianceEntityExtractor:\n",
    "    def __init__(self, ner_pipeline: RegulatoryNERPipeline):\n",
    "        self.ner_pipeline = ner_pipeline\n",
    "        # Patterns for extracting specific regulatory entities:\n",
    "        # 'emission_limits': Matches numbers (with commas/decimals) followed by units like tons/tonnes/tCO2e and emission keywords.\n",
    "        # 'financial_penalties': Matches dollar and euro  amounts (with commas/decimals) and optional multipliers like million/thousand/k/m.\n",
    "        # 'percentages': Matches numbers (with decimals) followed by a percent sign.\n",
    "        # 'dates': Matches dates in the format \" Day Month Year\" with optional prefixes like \"by\" or \"before\".\n",
    "        # 'deadlines': Matches deadline expressions/ exmaple = \n",
    "        # 'sectors': Matches sector or industry labels followed by a single uppercase letter/ example = Sector B\n",
    "        # 'standards': Matches regulatory standards / example = CBAM\n",
    "        self.patterns = {\n",
    "            'emission_limits': r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:t|tons?|tonnes?)?(?:\\s*CO2e?|carbon|emissions?)',\n",
    "            'financial_penalties': r'[\\$](\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:million|thousand|k|m)?',\n",
    "            'percentages': r'(\\d+(?:\\.\\d+)?)\\s*%',\n",
    "            'dates': r'(?:by\\s+|before\\s+)?\\d{1,2}\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December),?\\s+\\d{4}',\n",
    "            'deadlines': r'(?:deadline|due|by|before|until)\\s+(?:is\\s+)?([^.]+)',\n",
    "            'sectors': r'(?:sector|industry)\\s+([A-Z])\\b',\n",
    "            'standards': r'(Euro\\s+\\d+|CBAM|SEC\\s+filing\\s+\\d+-[A-Z]+)',\n",
    "        }\n",
    "    \n",
    "    def extract_compliance_entities(self, text: str) -> Dict:\n",
    "        results = {\n",
    "            'organizations': [],\n",
    "            'dates': [],\n",
    "            'emission_limits': [],\n",
    "            'financial_penalties': [],\n",
    "            'percentages': [],\n",
    "            'locations': [],\n",
    "            'persons': [],\n",
    "            'standards': [],\n",
    "            'sectors': [],\n",
    "            'raw_entities': []\n",
    "        }\n",
    "        try:\n",
    "            entities = self.ner_pipeline.extract_entities(text)\n",
    "            results['raw_entities'] = entities\n",
    "            #we extract the basic entities from the pipeline\n",
    "            for entity in entities:\n",
    "                entity_type = entity.get('entity_group', '').upper()\n",
    "                entity_text = entity.get('word', '').strip()\n",
    "                confidence = entity.get('score', 0)\n",
    "                if confidence < 0.5:\n",
    "                    continue\n",
    "                if entity_type in ['ORG', 'ORGANIZATION']:\n",
    "                    results['organizations'].append({\n",
    "                        'text': entity_text,\n",
    "                        'confidence': confidence,\n",
    "                        'start': entity.get('start', 0),\n",
    "                        'end': entity.get('end', 0)\n",
    "                    })\n",
    "                elif entity_type in ['DATE', 'TIME']:\n",
    "                    results['dates'].append({\n",
    "                        'text': entity_text,\n",
    "                        'confidence': confidence,\n",
    "                        'start': entity.get('start', 0),\n",
    "                        'end': entity.get('end', 0)\n",
    "                    })\n",
    "                elif entity_type in ['LOC', 'LOCATION']:\n",
    "                    results['locations'].append({\n",
    "                        'text': entity_text,\n",
    "                        'confidence': confidence,\n",
    "                        'start': entity.get('start', 0),\n",
    "                        'end': entity.get('end', 0)\n",
    "                    })\n",
    "                elif entity_type in ['PER', 'PERSON']:\n",
    "                    results['persons'].append({\n",
    "                        'text': entity_text,\n",
    "                        'confidence': confidence,\n",
    "                        'start': entity.get('start', 0),\n",
    "                        'end': entity.get('end', 0)\n",
    "                    })\n",
    "            #we extract the specific/custom entites we created using regular expressions \n",
    "            self._extract_pattern_entities(text, results)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return results\n",
    "    \n",
    "    def _extract_pattern_entities(self, text: str, results: Dict):\n",
    "        emission_matches = re.finditer(self.patterns['emission_limits'], text, re.IGNORECASE)\n",
    "        for match in emission_matches:\n",
    "            results['emission_limits'].append({\n",
    "                'text': match.group(0),\n",
    "                'value': match.group(1),\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "        penalty_matches = re.finditer(self.patterns['financial_penalties'], text, re.IGNORECASE)\n",
    "        for match in penalty_matches:\n",
    "            results['financial_penalties'].append({\n",
    "                'text': match.group(0),\n",
    "                'value': match.group(1),\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "        percentage_matches = re.finditer(self.patterns['percentages'], text)\n",
    "        for match in percentage_matches:\n",
    "            results['percentages'].append({\n",
    "                'text': match.group(0),\n",
    "                'value': match.group(1),\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "        date_matches = re.finditer(self.patterns['dates'], text, re.IGNORECASE)\n",
    "        for match in date_matches:\n",
    "            results['dates'].append({\n",
    "                'text': match.group(0),\n",
    "                'type': 'pattern_extracted',\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "        sector_matches = re.finditer(self.patterns['sectors'], text, re.IGNORECASE)\n",
    "        for match in sector_matches:\n",
    "            results['sectors'].append({\n",
    "                'text': match.group(0),\n",
    "                'sector': match.group(1),\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "        standard_matches = re.finditer(self.patterns['standards'], text, re.IGNORECASE)\n",
    "        for match in standard_matches:\n",
    "            results['standards'].append({\n",
    "                'text': match.group(0),\n",
    "                'standard': match.group(1),\n",
    "                'start': match.start(),\n",
    "                'end': match.end()\n",
    "            })\n",
    "\n",
    "compliance_extractor = ComplianceEntityExtractor(ner_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23b4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_compliance_rules(text: str) -> Dict:\n",
    "    try:\n",
    "        results = compliance_extractor.extract_compliance_entities(text)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "results = extract_compliance_rules(regulatory_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f6caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compliance_patterns(results: Dict) -> Dict:\n",
    "    analysis = {\n",
    "        'summary': {},\n",
    "        'insights': [],\n",
    "        'key_deadlines': [],\n",
    "        'financial_exposure': 0\n",
    "    }\n",
    "    analysis['summary'] = {\n",
    "        'total_organizations': len(results.get('organizations', [])),\n",
    "        'total_dates': len(results.get('dates', [])),\n",
    "        'total_emission_limits': len(results.get('emission_limits', [])),\n",
    "        'total_financial_penalties': len(results.get('financial_penalties', [])),\n",
    "        'total_locations': len(results.get('locations', [])),\n",
    "        'total_standards': len(results.get('standards', []))\n",
    "    }\n",
    "    for penalty in results.get('financial_penalties', []):\n",
    "        try:\n",
    "            value = float(penalty['value'].replace(',', ''))\n",
    "            analysis['financial_exposure'] += value\n",
    "        except (ValueError, KeyError):\n",
    "            continue\n",
    "\n",
    "    for date in results.get('dates', []):\n",
    "        if any(keyword in date['text'].lower() for keyword in ['deadline', 'by', 'before', 'due']):\n",
    "            analysis['key_deadlines'].append(date['text'])\n",
    "\n",
    "    if analysis['summary']['total_organizations'] > 0:\n",
    "        analysis['insights'].append(f\"Found {analysis['summary']['total_organizations']} organizations mentioned in regulatory text\")\n",
    "    if analysis['summary']['total_emission_limits'] > 0:\n",
    "        analysis['insights'].append(f\"Identified {analysis['summary']['total_emission_limits']} emission limits/targets\")\n",
    "    if analysis['financial_exposure'] > 0:\n",
    "        analysis['insights'].append(f\"Total financial exposure: ${analysis['financial_exposure']:,.2f}\")\n",
    "    if len(analysis['key_deadlines']) > 0:\n",
    "        analysis['insights'].append(f\"Found {len(analysis['key_deadlines'])} critical deadlines\")\n",
    "        \n",
    "    return analysis\n",
    "\n",
    "\n",
    "#we can add another regulatory text to analyse then we combine the results \n",
    "analysis = analyze_compliance_patterns(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52244dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compliance_dataframe(results: Dict) -> pd.DataFrame:\n",
    "    all_entities = []\n",
    "    for entity_type, entities in results.items():\n",
    "        if entity_type == 'raw_entities':\n",
    "            continue\n",
    "        for entity in entities:\n",
    "            entity_row = {\n",
    "                'entity_type': entity_type,\n",
    "                'text': entity.get('text', ''),\n",
    "                'confidence': entity.get('confidence', None),\n",
    "                'start': entity.get('start', None),\n",
    "                'end': entity.get('end', None),\n",
    "                'value': entity.get('value', None),\n",
    "                'sector': entity.get('sector', None),\n",
    "                'standard': entity.get('standard', None)\n",
    "            }\n",
    "            all_entities.append(entity_row)\n",
    "    return pd.DataFrame(all_entities)\n",
    "\n",
    "df = create_compliance_dataframe(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ee2fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to: regulatory_entities_extraction.csv\n",
      "Complete results saved to: regulatory_compliance_results.json\n"
     ]
    }
   ],
   "source": [
    "#we save the dataframe to a CSV file \n",
    "csv_filename = \"regulatory_entities_extraction.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Entities saved to: {csv_filename}\")\n",
    "\n",
    "#we save the results and analysis to a JSON file\n",
    "results_filename = \"regulatory_compliance_results.json\"\n",
    "with open(results_filename, 'w') as f:\n",
    "    json_results = {\n",
    "        'extraction_results': results,\n",
    "        'analysis': analysis,\n",
    "        'model_info': {\n",
    "            'model_name': ner_extractor.model_name,\n",
    "            'extraction_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Complete results saved to: {results_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
