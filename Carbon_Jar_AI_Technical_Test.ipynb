{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYg4tQVFxs3y"
      },
      "source": [
        "## **Welcome to the Final Step**\n",
        "Welcome to the final, and most exciting, step! We're thrilled you're here. At Carbon Jar, we're not just writing code; we're building an **AI-powered future for carbon accounting in North Africa**, starting from a blank canvas. This is your chance to show us how your AI expertise can help us solve some truly unique and impactful problems.\n",
        "\n",
        "This isn't your standard exam. Think of it as a collaborative challenge ‚Äì a glimpse into the groundbreaking work we'll do together.\n",
        "\n",
        "**Your AI Mission:**\n",
        "\n",
        "* **Time:** You have **up to 25 hours** over **5 days**.\n",
        "* **Our Take on Time:** This is a marathon, not just a sprint! 25 hours for these deep AI challenges is demanding. **You are not expected to complete everything.** Please, focus your energy. We want to see 5-7 questions answered *brilliantly*, with working code and clear explanations, rather than 20 surface-level attempts. **Depth, creativity, and robust AI implementation** are what we're looking for.\n",
        "* **Integrity:** Our culture is built on trust and innovation. We need to see *your* authentic problem-solving skills ‚Äì **no AI assistance (like Copilot, ChatGPT, etc.)** in completing this, please. We want to see *your* intelligence shine!\n",
        "* **Submission:** Use **GitHub**. Organize your code, notebooks, and explanations logically. Include a `README.md` to guide us through your solutions. Share the link!\n",
        "* **Questions?** Building the future means asking questions. We're here to help ‚Äì reach out if anything is unclear.\n",
        "\n",
        "**Tips for Success:**\n",
        "* **Prioritize:** Choose the AI challenges that ignite your passion and showcase your best work.\n",
        "* **Build Smart:** Write clean, well-commented Python/ML code. Think about reproducibility and MLOps.\n",
        "* **Explain Your 'Why':** Why *this* model? *This* architecture? *This* validation strategy?\n",
        "* **Debug with Grit:** AI code rarely works the first time. Show us your debugging prowess.\n",
        "* **Stay Well:** This is tough. Take breaks. We value your well-being!\n",
        "\n",
        "Ready to apply AI to one of the world's most critical challenges? Let's begin!\n",
        "\n",
        "---\n",
        "\n",
        "## The AI Build-from-Scratch Challenges üí°\n",
        "\n",
        "### 1. Designing AI-Powered Emissions Anomaly Detection üìà\n",
        "\n",
        "**Scenario:** We need a real-time pipeline to detect anomalies in diverse emissions data streams, considering seasonality and noise.\n",
        "\n",
        "**Tasks:**\n",
        "* **Architect & Build:** Design an end-to-end pipeline. **Implement a basic `LSTM Autoencoder` in `PyTorch` or `TensorFlow`.** The provided snippet has a flawed structure. **Fix the model architecture** (ensure input/output shapes match, layers are appropriate) and **write a basic training loop** (using simulated data) that calculates reconstruction error.\n",
        "* **Refine:** Explain your choices and how you'd handle preprocessing (scaling, detrending) and feature extraction for real-world deployment.\n",
        "**Code Sample (`PyTorch` LSTM AE - Needs Fixing!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azmlFtHKxs32"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_AE(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=32, n_layers=1):\n",
        "        super(LSTM_AE, self).__init__()\n",
        "        # PROBLEM 1: Encoder/Decoder might be mismatched or too simple.\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        # PROBLEM 2: How does the decoder reconstruct the original sequence length & dim?\n",
        "        self.decoder = nn.LSTM(hidden_dim, input_dim, n_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # PROBLEM 3: How to get the *final* hidden state to feed the decoder?\n",
        "        # PROBLEM 4: Decoder needs an initial input. How to provide it?\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        outputs, _ = self.decoder(hidden.permute(1, 0, 2)) # This won't work.\n",
        "        return outputs\n",
        "\n",
        "# PROBLEM 5: Needs a training loop: Generate sample data, run model, calc MSE loss.\n",
        "# Our Take: LSTMs are great for sequences, but building a working Autoencoder\n",
        "# requires careful architecture. Fix this model and show us a basic training setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0QJIYFuxs35"
      },
      "source": [
        "---\n",
        "\n",
        "### 2. AI-Based Imputation of Missing Emissions Data üíß\n",
        "\n",
        "**Scenario:** Our datasets have missing values. We need sophisticated imputation.\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement Advanced Imputation:** Simple mean/median isn't enough. **Implement `sklearn.impute.IterativeImputer`** on a sample dataset (create one with missing values).\n",
        "* **Design (GAN):** While not coding a full GAN, **sketch the `PyTorch`/`TensorFlow` architecture** for a `Generator` and `Discriminator` suitable for generating synthetic *tabular* emissions data. Highlight the key layers and activation functions.\n",
        "* **Validate:** Explain how you'd validate the quality of imputed data (e.g., distribution checks, downstream model performance).\n",
        "**Code Sample (Python `sklearn` - Needs Advanced Approach!):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gB6dJjsxs36"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample data with missing values\n",
        "X = np.array([[10., 2., 30.], [4., np.nan, 60.], [np.nan, 8., 90.], [10., 5., np.nan]])\n",
        "\n",
        "# PROBLEM: Simple imputation doesn't capture relationships.\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "print(\"Simple Imputed:\\n\", X_imputed)\n",
        "\n",
        "# TASK: Implement IterativeImputer here & design a GAN architecture.\n",
        "# Our Take: We need imputation that understands data relationships. Show us\n",
        "# `IterativeImputer` in action and a blueprint for a GAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Tr6M-Jxs39"
      },
      "source": [
        "---\n",
        "\n",
        "### 3. Feature Engineering Automation üõ†Ô∏è\n",
        "\n",
        "**Scenario:** We need engineered features (rolling means, ratios) for better anomaly detection.\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement Feature Functions:** Write `pandas` functions to create: 1) 7-day rolling averages of emissions. 2) Emission intensity (emissions / production_volume). 3) Day-of-week/Month features.\n",
        "* **Implement Feature Selection:** Add a step using `sklearn.feature_selection.SelectKBest` or `RFE` to **choose the most important features** based on a (simulated) target variable.\n",
        "* **Validate:** Explain how you'd use `SHAP` to validate the importance of your engineered features *after* training a simple model (e.g., `RandomForestRegressor`).\n",
        "**Code Sample (Python `pandas` - Needs More Features & Selection!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rc3SvfAxs3-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def basic_features(df):\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    # PROBLEM: Only basic time features. Needs rolling stats, ratios, etc.\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    return df\n",
        "\n",
        "# df = pd.DataFrame({'timestamp': pd.to_datetime(['2024-01-01', '2024-01-02']),\n",
        "#                    'emissions': [100, 110], 'production': [50, 55]})\n",
        "# TASK: Add rolling mean, intensity, and feature selection steps.\n",
        "# Our Take: Raw data isn't enough. Show us how you'd engineer insightful\n",
        "# features and select the best ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBY3PpJKxs3_"
      },
      "source": [
        "---\n",
        "\n",
        "### 4. Modular AI-Augmented LCA Model ‚ôªÔ∏è\n",
        "\n",
        "**Scenario:** We're building an AI-enhanced Life Cycle Assessment (LCA) model.\n",
        "\n",
        "**Tasks:**\n",
        "* **Refactor to Modules:** The `Python` class below is monolithic. **Refactor it into separate classes** (`DataIngestor`, `EmissionFactorMapper`, `SupplyChainModel`, `ImpactCalculator`, `UncertaintyAnalyzer`).\n",
        "* **Implement Uncertainty:** Add a basic **Monte Carlo simulation** method to the `UncertaintyAnalyzer` class to estimate the range of potential LCA results, given uncertainty ranges for emission factors.\n",
        "* **Design:** Explain how AI (e.g., NLP for factor extraction, GNNs for supply chains) could enhance each module.\n",
        "**Code Sample (Python Class - Needs Refactoring & Monte Carlo!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqVLuzF2xs4B"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class MonolithicLCA:\n",
        "    def __init__(self, product_data):\n",
        "        self.data = product_data\n",
        "        # PROBLEM: All logic is crammed here - ingestion, mapping, calculation...\n",
        "        self.factors = {'steel': 2.1, 'transport': 0.15}\n",
        "\n",
        "    def run_lca(self):\n",
        "        steel_emissions = self.data['steel_kg'] * self.factors['steel']\n",
        "        transport_emissions = self.data['transport_km'] * self.factors['transport']\n",
        "        total = steel_emissions + transport_emissions\n",
        "        # PROBLEM: No modularity, no uncertainty handling.\n",
        "        print(f\"Total Emissions: {total} tCO2e\")\n",
        "        return total\n",
        "\n",
        "# data = {'steel_kg': 1000, 'transport_km': 500}\n",
        "# lca = MonolithicLCA(data); lca.run_lca()\n",
        "# Our Take: A complex LCA needs modularity. Refactor this and add uncertainty analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_pvYCq8xs4D"
      },
      "source": [
        "---\n",
        "\n",
        "### 5. Reinforcement Learning for Decarbonization ü§ñ\n",
        "\n",
        "**Scenario:** We want to use RL to recommend cost-effective decarbonization actions.\n",
        "\n",
        "**Tasks:**\n",
        "* **Define the RL Problem:** Formalize states, actions, and rewards for this scenario.\n",
        "* **Fix the Environment:** The `Gymnasium` environment below is incomplete. **Fix the `step` method** to correctly update the state and calculate a meaningful reward (e.g., `reward = emission_reduction - action_cost`). **Fix the `reset` method.**\n",
        "* **Implement Training:** Write a *basic* training loop using `Stable-Baselines3` (e.g., `PPO`) to train an agent on your fixed environment.\n",
        "**Code Sample (`Gymnasium`/Python - Needs Fixing!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZcJJZRxs4E"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class DecarbEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(DecarbEnv, self).__init__()\n",
        "        # Actions: 0=Invest Renewables, 1=Improve Efficiency, 2=Do Nothing\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        # State: [CurrentEmissions, BudgetRemaining]\n",
        "        self.observation_space = spaces.Box(low=0, high=1000, shape=(2,), dtype=np.float32)\n",
        "        self.state = [1000, 500]\n",
        "\n",
        "    def step(self, action):\n",
        "        emissions, budget = self.state\n",
        "        # PROBLEM 1: Reward logic is missing/trivial.\n",
        "        # PROBLEM 2: State update logic is missing\n",
        "        # PROBLEM 3: `terminated` and `truncated` logic is missing.\n",
        "        reward = -1\n",
        "        terminated = budget <= 0\n",
        "        truncated = False\n",
        "        info = {}\n",
        "        return np.array(self.state), reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # PROBLEM 4: Doesn't properly reset state.\n",
        "        self.state = [1000, 500]\n",
        "        return np.array(self.state), {}\n",
        "\n",
        "# TASK: Fix `step` and `reset`, then write a basic Stable-Baselines3 training script.\n",
        "# Our Take: RL is powerful but needs a well-defined environment. Fix this one\n",
        "# and show us how to start training an agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mjZIpEBxs4G"
      },
      "source": [
        "---\n",
        "\n",
        "### 6. Explainability for Audit-Ready AI üîç\n",
        "\n",
        "**Scenario:** Our anomaly detection model needs transparent, audit-proof explanations.\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement `SHAP`:** Assume you have a trained tree-based model (e.g., `XGBoost` - simulate one or train a basic one). **Write `Python` code using the `shap` library** to: 1) Create an `Explainer`. 2) Calculate `SHAP` values for a few sample anomaly instances. 3) **Generate a `shap.force_plot`** (you'll need to describe how you'd save/display it).\n",
        "* **Explain:** How would you translate these `SHAP` values into human-readable explanations for an audit report?\n",
        "**Code Sample (Python `SHAP` - Needs Implementation!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chcLAVSVxs4G"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Simulate a trained model and data\n",
        "X_train = pd.DataFrame(np.random.rand(100, 5), columns=[f'feat_{i}' for i in range(5)])\n",
        "y_train = pd.Series(np.random.rand(100) > 0.9) # Binary target (anomaly)\n",
        "model = xgb.XGBClassifier().fit(X_train, y_train)\n",
        "X_test_anomalies = X_train[y_train == 1].head(3)\n",
        "\n",
        "# PROBLEM: Needs SHAP implementation.\n",
        "# 1. Create a shap.Explainer (e.g., TreeExplainer)\n",
        "# 2. Calculate shap_values for X_test_anomalies\n",
        "# 3. Show how to generate a force_plot or summary_plot\n",
        "\n",
        "print(\"SHAP values need to be calculated and plotted here.\")\n",
        "# Our Take: Black boxes won't work for audits. Show us how to use SHAP to\n",
        "# explain our AI's decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU_cgJkbxs4H"
      },
      "source": [
        "---\n",
        "\n",
        "### 7. Embedding Domain Knowledge into AI üß†\n",
        "\n",
        "**Scenario:** We need our AI models to respect known regulations (e.g., emission caps).\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement Post-Processing Constraints:** Take a (simulated) ML model's output (e.g., predicted emissions). **Write a `Python` function** that acts as a post-processing step: If the prediction exceeds a known `REGULATORY_CAP`, it should either cap the prediction at the limit *or* flag it as a high-priority \"Regulatory Breach\" anomaly, while also providing the original prediction.\n",
        "* **Discuss:** How else could you embed this knowledge (feature engineering, hybrid models)? What are the pros/cons?\n",
        "**Code Sample (Python - Needs Constraint Logic!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBqsIJYnxs4H"
      },
      "outputs": [],
      "source": [
        "REGULATORY_CAP = 500.0 # Example cap\n",
        "\n",
        "def predict_emissions_ml(input_data):\n",
        "    # Simulates an ML model prediction\n",
        "    return random.uniform(450, 550)\n",
        "\n",
        "def apply_domain_knowledge(prediction):\n",
        "    # PROBLEM: Needs logic to check against REGULATORY_CAP and flag/cap.\n",
        "    if prediction > REGULATORY_CAP:\n",
        "        print(f\"Potential Breach! Predicted: {prediction}, Cap: {REGULATORY_CAP}\")\n",
        "        # Return something structured here.\n",
        "    return {\"final_value\": prediction, \"breach_flag\": False}\n",
        "\n",
        "# Our Take: Pure ML can be naive. We need to ensure our AI respects\n",
        "# real-world rules. Show us a simple way to enforce a constraint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWjenrWoxs4I"
      },
      "source": [
        "---\n",
        "\n",
        "### 8. NLP Pipeline for Regulatory Document Parsing üìÑ\n",
        "\n",
        "**Scenario:** We need to extract compliance rules (e.g., limits, reporting dates) from regulatory PDFs.\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement `NER` with `Hugging Face`:** The `Python` script below sets up a basic `Hugging Face` pipeline. **Enhance it** to: 1) Use a specific, pre-trained `NER` model (e.g., `dslim/bert-base-NER`). 2) Process a sample text (provided). 3) **Extract and print entities** relevant to compliance (like `ORG`, `DATE`, and potentially custom ones if you describe how you'd fine-tune). 4) Add error handling.\n",
        "**Code Sample (Python `Hugging Face` - Needs `NER` & Processing!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCcCSzeyxs4I"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# PROBLEM 1: Using a generic 'sentiment-analysis' pipeline. Needs NER.\n",
        "# PROBLEM 2: Needs to process text and extract specific entities.\n",
        "nlp_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "regulatory_text = \"\"\"\n",
        "Carbon Jar Inc. must report its Scope 1 emissions by March 31, 2026.\n",
        "The limit for Sector B in Egypt is 50,000 tCO2e.\n",
        "\"\"\"\n",
        "\n",
        "def extract_compliance_rules(text):\n",
        "    print(\"NLP pipeline needs to be configured for NER and run here.\")\n",
        "    # results = nlp_pipeline(text)\n",
        "    # Filter and print ORG, DATE, and maybe numeric limits.\n",
        "    return {}\n",
        "\n",
        "# extract_compliance_rules(regulatory_text)\n",
        "# Our Take: Regulatory docs are dense. Show us how to use Transformers to\n",
        "# pull out the critical pieces automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I627WZKQxs4I"
      },
      "source": [
        "---\n",
        "\n",
        "### 9. AI-Driven Generation of Compliance Reports üìù\n",
        "\n",
        "**Scenario:** We need to auto-generate audit reports using AI-driven insights.\n",
        "\n",
        "**Tasks:**\n",
        "* **Build a `Jinja2` Report Generator:** The `Python` script uses `Jinja2` but has a basic template. **Enhance both the script and the template** to: 1) Include sections for *summary statistics*, *anomaly lists* (with explanations), and *regulatory references*. 2) Dynamically insert a simple `matplotlib` plot (save to base64 and embed).\n",
        "**Code Sample (Python `Jinja2` - Needs Enhancing!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpQOFuW6xs4J"
      },
      "outputs": [],
      "source": [
        "from jinja2 import Environment, FileSystemLoader\n",
        "import base64; from io import BytesIO; import matplotlib.pyplot as plt\n",
        "\n",
        "env = Environment(loader=FileSystemLoader('.'))\n",
        "template = env.from_string(\"\"\"\n",
        "<h1>Emissions Report: {{ month }}</h1>\n",
        "<p>Total Emissions: {{ total_emissions }}</p>\n",
        "\"\"\")\n",
        "\n",
        "def generate_report(data):\n",
        "    # Simulate a plot\n",
        "    fig, ax = plt.subplots(); ax.bar(['A'], [data['total_emissions']]); buf = BytesIO()\n",
        "    fig.savefig(buf, format='png'); buf.seek(0); img_b64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "\n",
        "    # PROBLEM: Needs to pass plot data & anomaly details to template.\n",
        "    html = template.render(month=\"Jan 2025\", total_emissions=data['total_emissions'])\n",
        "    with open(\"report.html\", \"w\") as f: f.write(html)\n",
        "\n",
        "# generate_report({'total_emissions': 12345, 'anomalies': [{'id':1, 'reason':'High'}]})\n",
        "# Our Take: Reports need to be rich and dynamic. Enhance this generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGcmnR3xxs4J"
      },
      "source": [
        "---\n",
        "\n",
        "### 10. MLOps: Production Deployment & Monitoring üö¢\n",
        "\n",
        "**Scenario:** We need to deploy our AI models on K8s and monitor them.\n",
        "\n",
        "**Tasks:**\n",
        "* **Fix the `Seldon Core` Manifest:** The `YAML` below tries to deploy a model with `Seldon Core` but has **structural errors and missing monitoring setup**. **Fix the `YAML`** to correctly define a predictor, add basic resource requests/limits, and include annotations for `Prometheus` scraping.\n",
        "* **Design Monitoring:** Explain how you'd implement data drift detection (conceptually or with pseudo-code).\n",
        "**Code Sample (K8s `Seldon` YAML - Needs Fixing!):*\n",
        "```yaml\n",
        "apiVersion: machinelearning.seldon.io/v1\n",
        "kind: SeldonDeployment\n",
        "metadata:\n",
        "  name: emission-anomaly-detector\n",
        "spec:\n",
        "  name: detector\n",
        "  predictors:\n",
        "    - componentSpecs:\n",
        "        - spec:\n",
        "            containers:\n",
        "              - name: anomaly-model\n",
        "                image: my-anomaly-model:0.1\n",
        "                # PROBLEM 1: Missing resource requests/limits.\n",
        "                # PROBLEM 2: Missing port definition.\n",
        "      graph:\n",
        "        name: anomaly-model\n",
        "        type: MODEL\n",
        "        endpoint:\n",
        "          type: REST\n",
        "      name: main-predictor\n",
        "      replicas: 1\n",
        "  # PROBLEM 3: Missing annotations/setup for Prometheus monitoring.\n",
        "```\n",
        "**Our Take:** Deploying models is just the start. We need robust, monitored deployments. Fix this manifest.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. Privacy-Preserving AI Techniques üõ°Ô∏è\n",
        "\n",
        "**Scenario:** Emissions data is sensitive; we need privacy-preserving AI.\n",
        "\n",
        "**Tasks:**\n",
        "* **Integrate `Opacus` (or `TF Privacy`):** Take a standard `PyTorch` (or TF) training loop (provide a basic one). **Modify it to integrate `Opacus`** (or TF Privacy) to apply differential privacy. This involves using `Opacus`'s `PrivacyEngine`. Explain the key parameters (`epsilon`, `delta`, `max_grad_norm`).\n",
        "**Code Sample (Python `PyTorch` - Needs `Opacus`!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzSqve3Gxs4K"
      },
      "outputs": [],
      "source": [
        "import torch; import torch.nn as nn; import torch.optim as optim\n",
        "\n",
        "# Assume model, dataloader, criterion are defined.\n",
        "model = nn.Linear(10, 1); dataloader = ...; criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# PROBLEM: Standard training loop, no differential privacy.\n",
        "def train_standard(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for data, target in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"Standard training epoch done.\")\n",
        "\n",
        "# TASK: Refactor this to use Opacus's PrivacyEngine.\n",
        "# You'll need to `PrivacyEngine(model, ...).attach(optimizer)`.\n",
        "# Our Take: Privacy is crucial. Show us how to start implementing\n",
        "# differential privacy in our training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n3Ps6T6xs4K"
      },
      "source": [
        "---\n",
        "\n",
        "### 12. Addressing Class Imbalance ‚öñÔ∏è\n",
        "\n",
        "**Scenario:** Anomalies are rare, making our models biased.\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement `SMOTE`:** The `scikit-learn` script below trains on imbalanced data. **Add a step using `imbalanced-learn`'s `SMOTE`** to oversample the minority class *before* training.\n",
        "* **Compare Metrics:** Explain why `Accuracy` is a bad metric here and why `Precision-Recall` or `F1` are better. Calculate these metrics *before* and *after* `SMOTE`.\n",
        "**Code Sample (Python `sklearn` - Needs `SMOTE`!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIaSPzvMxs4K"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, weights=[0.95, 0.05], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# PROBLEM: Trains on imbalanced data.\n",
        "# TASK: Insert SMOTE here (from imblearn.over_sampling import SMOTE)\n",
        "#       sm = SMOTE(); X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "#       Then train on X_train_sm, y_train_sm.\n",
        "\n",
        "model = LogisticRegression(); model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy (Before SMOTE): {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"F1 Score (Before SMOTE): {f1_score(y_test, y_pred)}\") # Likely poor.\n",
        "# Our Take: Imbalance plagues anomaly detection. Show us a standard technique to fix it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHFQbdV-xs4L"
      },
      "source": [
        "---\n",
        "\n",
        "### 13. Transfer Learning Across Regions ‚úàÔ∏è\n",
        "\n",
        "**Scenario:** We have lots of French data but less from Egypt/Morocco. Can we transfer learning?\n",
        "\n",
        "**Tasks:**\n",
        "* **Implement Fine-Tuning:** The `PyTorch`/`TensorFlow` script below defines a model. **Show how you would modify a training script** to: 1) Load pre-trained weights (simulate this). 2) *Freeze* the early layers. 3) *Fine-tune* only the final layers on a new (simulated) smaller dataset representing Egypt.\n",
        "**Code Sample (Python `PyTorch` - Needs Fine-Tuning Logic!):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFZ6qM66xs4M"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EmissionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.base = nn.Sequential(nn.Linear(10, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU())\n",
        "        self.classifier = nn.Linear(32, 1)\n",
        "    def forward(self, x): return self.classifier(self.base(x))\n",
        "\n",
        "model = EmissionModel()\n",
        "# model.load_state_dict(torch.load('france_weights.pth')) # Simulate loading\n",
        "\n",
        "# PROBLEM: Needs code to freeze base layers & train only the classifier.\n",
        "# for param in model.base.parameters(): param.requires_grad = False\n",
        "# optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "# ... then write the fine-tuning loop ...\n",
        "# Our Take: Transfer learning can save us time. Show us the core Pytorch/TF\n",
        "# mechanics of freezing layers and fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJWMB4qsxs4M"
      },
      "source": [
        "---\n",
        "\n",
        "### 14. Continuous Learning & MLOps Automation üîÑ\n",
        "\n",
        "**Scenario:** Emissions patterns evolve; we need continuous model updates.\n",
        "\n",
        "**Tasks:** **Design an MLOps `Airflow` DAG.** While not coding every operator, **design and provide the `Python` structure for an `Airflow` DAG** that automates an ML lifecycle. Include (as `BashOperator` or `PythonOperator` placeholders): 1) Data Ingestion. 2) Data Validation (`Great Expectations`). 3) Feature Engineering. 4) Model Retraining. 5) Model Validation (vs. current prod model). 6) Conditional Deployment (only if new model is better). 7) Monitoring Hook.\n",
        "**Code Sample (`Airflow` DAG - Needs MLOps Structure!):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTBX2fOtxs4M"
      },
      "outputs": [],
      "source": [
        "from airflow.models.dag import DAG; from airflow.operators.bash import BashOperator; from airflow.utils.dates import days_ago\n",
        "\n",
        "with DAG(dag_id='mlops_continuous_learning', start_date=days_ago(1), schedule_interval='@weekly') as dag:\n",
        "    ingest = BashOperator(task_id='ingest_data', bash_command='echo \"Ingest\"')\n",
        "    # PROBLEM: Needs a full MLOps flow: Validate -> FeatEng -> Train -> ValidateModel -> Deploy\n",
        "    deploy = BashOperator(task_id='deploy_model', bash_command='echo \"Deploy\"')\n",
        "    ingest >> deploy\n",
        "# Our Take: We need MLOps automation. Sketch a robust Airflow DAG for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExQRyuGxs4N"
      },
      "source": [
        "---\n",
        "\n",
        "### 15. Scenario Simulation for Decarbonization üé≤\n",
        "\n",
        "**Scenario:** Clients want to simulate \"what-if\" decarbonization scenarios.\n",
        "\n",
        "**Tasks:** **Implement Monte Carlo Simulation.** The `Python` script simulates a *single* scenario. **Enhance it to run a Monte Carlo simulation (e.g., 1000 runs)** where `cost_reduction` and `emission_cut` have a defined *distribution* (e.g., normal distribution using `numpy.random.normal`) instead of being fixed. Collect and plot a histogram of the final `net_benefit`.\n",
        "**Code Sample (Python - Needs Monte Carlo!):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G16VZAS_xs4N"
      },
      "outputs": [],
      "source": [
        "import numpy as np; import matplotlib.pyplot as plt\n",
        "\n",
        "def simulate_scenario(investment):\n",
        "    # PROBLEM: Deterministic, no uncertainty.\n",
        "    cost_reduction_factor = 0.1\n",
        "    emission_cut_factor = 0.05\n",
        "    emission_value = 50 # Value per ton cut\n",
        "\n",
        "    cost_reduction = investment * cost_reduction_factor\n",
        "    emission_cut = investment * emission_cut_factor\n",
        "    net_benefit = (emission_cut * emission_value) - (investment - cost_reduction)\n",
        "    return net_benefit\n",
        "\n",
        "# investment = 100000\n",
        "# print(f\"Single Run Benefit: {simulate_scenario(investment)}\")\n",
        "# TASK: Run this 1000 times, varying factors with np.random.normal. Plot a histogram.\n",
        "# Our Take: Decisions need uncertainty analysis. Add Monte Carlo to this simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpNKFCA2xs4N"
      },
      "source": [
        "---\n",
        "\n",
        "### 16. Comprehensive AI Documentation & Model Cards üìñ\n",
        "\n",
        "**Scenario:** We need excellent docs for our AI models.\n",
        "\n",
        "**Tasks:** **Write Docstrings & a Model Card.** The `Python` script has a model with *no* docstrings. 1) **Add comprehensive `NumPy`-style docstrings** to the `__init__` and `predict` methods. 2) **Create a `Markdown` file (`model_card.md`)** for this model, including sections like Model Details, Intended Use, Training Data, Evaluation Data, Metrics, Ethical Considerations, and Caveats.\n",
        "**Code Sample (Python - Needs Docstrings!):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m3O93Q_xs4N"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "class EmissionPredictor:\n",
        "    # PROBLEM: No docstrings!\n",
        "    def __init__(self, n_estimators=100):\n",
        "        self.model = RandomForestRegressor(n_estimators=n_estimators)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# TASK: Add docstrings & create model_card.md.\n",
        "# Our Take: Good docs & model cards are essential for transparency and teamwork."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ73yULCxs4N"
      },
      "source": [
        "---\n",
        "\n",
        "*(The following 4 questions are less code-intensive, focusing more on methodology, design, and experience.)*\n",
        "\n",
        "### 17. Backend API Design for AI Models (Design)\n",
        "\n",
        "**Scenario:** How should we expose our AI models (anomaly detection, prediction) via `REST APIs`?\n",
        "\n",
        "**Tasks:** Design a `REST API` structure. Consider: What endpoints are needed? How should inputs (features) and outputs (predictions, explanations) be structured (`JSON`)? How should we handle asynchronous predictions for long-running models? How would you version these AI-specific APIs?\n",
        "\n",
        "---\n",
        "\n",
        "### 18. Backend Resilience for AI Inference (Design)\n",
        "\n",
        "**Scenario:** Our AI inference service might fail due to model errors, resource limits, or transient issues.\n",
        "\n",
        "**Tasks:** How would you design resilience? Implement `Python` pseudo-code (or use `tenacity`) for retries. Describe how `circuit breakers` would work here. How would you implement `graceful degradation` (e.g., falling back to a simpler model or default value if the main AI fails)?\n",
        "\n",
        "---\n",
        "\n",
        "### 19. Time Management Under Pressure (Experience)\n",
        "\n",
        "**Scenario:** You have 4 hours for an AI model tuning challenge and get stuck.\n",
        "\n",
        "**Tasks:** Describe your approach: How do you manage time, prioritize (which hyperparameter to tune first?), know when an approach isn't working, and save time for review/reporting? Share a real-world AI/ML example.\n",
        "\n",
        "---\n",
        "\n",
        "### 20. AI Problem Solving and Debugging (Experience)\n",
        "\n",
        "**Scenario:** A production AI model's performance suddenly degrades (concept/data drift).\n",
        "\n",
        "**Tasks:** Outline your debugging methodology: How do you identify drift (monitoring tools, statistical tests)? How do you trace the cause (input data changes? infrastructure? model itself?)? How do you reproduce, fix (retrain? rollback?), validate, and document the fix?\n",
        "\n",
        "---\n",
        "\n",
        "This is your canvas, AI pioneer! We know it's challenging, but we believe it reflects the exciting work ahead. Show us your best thinking, your sharpest code, and your passion for using AI for good. We can't wait to see your solutions! Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
