{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa40ab1",
   "metadata": {},
   "source": [
    "### APPROACH 1 FOR TRANSFER LEARNING:\n",
    "In this approach, we load a pre-trained model (trained on the France dataset), freeze the base layers, and only fine-tune the classifier layer using the Egypt dataset. This allows us to leverage learned features from the source domain and adapt the model efficiently to the new target domain with limited training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154546cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e1e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5119516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(10, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(32, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.base(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99144f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmissionModel()\n",
    "# Simulate loading pre-trained weights\n",
    "# model.load_state_dict(torch.load('france_weights.pth'))\n",
    "\n",
    "for param in model.base.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d25af",
   "metadata": {},
   "source": [
    "**We can clearly see we succecsfully freezed all base layer and kept  classifier a trainabl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7d8818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "EmissionModel (EmissionModel)            [1, 10]              [1, 1]               --                   Partial\n",
       "├─Sequential (base)                      [1, 10]              [1, 32]              --                   False\n",
       "│    └─Linear (0)                        [1, 10]              [1, 64]              (704)                False\n",
       "│    └─ReLU (1)                          [1, 64]              [1, 64]              --                   --\n",
       "│    └─Linear (2)                        [1, 64]              [1, 32]              (2,080)              False\n",
       "│    └─ReLU (3)                          [1, 32]              [1, 32]              --                   --\n",
       "├─Linear (classifier)                    [1, 32]              [1, 1]               33                   True\n",
       "========================================================================================================================\n",
       "Total params: 2,817\n",
       "Trainable params: 33\n",
       "Non-trainable params: 2,784\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 10), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=20, row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7164b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1072f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_egypt = torch.randn(32, 10)\n",
    "y_egypt = torch.randn(32, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b33356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "X_egypt = X_egypt.to(device)\n",
    "y_egypt = y_egypt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed847df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9361\n",
      "Epoch 2, Loss: 0.9350\n",
      "Epoch 3, Loss: 0.9340\n",
      "Epoch 4, Loss: 0.9330\n",
      "Epoch 5, Loss: 0.9320\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_egypt)\n",
    "    loss = loss_fn(outputs, y_egypt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1acb3b",
   "metadata": {},
   "source": [
    "### APPROACH 2 FOR TRANSFER LEARNING:\n",
    "we freeze the layers when creating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2c369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9937a44",
   "metadata": {},
   "source": [
    "we create the model and we freeze inside this model (same model architecture that france data got trained on )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b38c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        model_weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1    \n",
    "        model = torchvision.models.resnet18(weights=model_weights)\n",
    "        self.encoder = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        self.embedding = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2, inplace=True),  \n",
    "            torch.nn.Linear(in_features=512, out_features=embedding_dim, bias=True) ,\n",
    "            torch.nn.BatchNorm1d(embedding_dim)   \n",
    "            )           \n",
    "        \n",
    "        for param in list(self.encoder.parameters())[:-4]:  \n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.normalize(\n",
    "        self.embedding(torch.flatten(self.encoder(x), 1)),\n",
    "        p=2, dim=1\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d63bfa",
   "metadata": {},
   "source": [
    "we can see the summary of the model. Majority of the encoder is frozen we left the last 2 layers in the encoder to increase generalization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb42d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "Resnet18 (Resnet18)                           [32, 3, 128, 128]    [32, 256]            --                   Partial\n",
       "├─Sequential (encoder)                        [32, 3, 128, 128]    [32, 512, 1, 1]      --                   Partial\n",
       "│    └─Conv2d (0)                             [32, 3, 128, 128]    [32, 64, 64, 64]     (9,408)              False\n",
       "│    └─BatchNorm2d (1)                        [32, 64, 64, 64]     [32, 64, 64, 64]     (128)                False\n",
       "│    └─ReLU (2)                               [32, 64, 64, 64]     [32, 64, 64, 64]     --                   --\n",
       "│    └─MaxPool2d (3)                          [32, 64, 64, 64]     [32, 64, 32, 32]     --                   --\n",
       "│    └─Sequential (4)                         [32, 64, 32, 32]     [32, 64, 32, 32]     --                   False\n",
       "│    │    └─BasicBlock (0)                    [32, 64, 32, 32]     [32, 64, 32, 32]     (73,984)             False\n",
       "│    │    └─BasicBlock (1)                    [32, 64, 32, 32]     [32, 64, 32, 32]     (73,984)             False\n",
       "│    └─Sequential (5)                         [32, 64, 32, 32]     [32, 128, 16, 16]    --                   False\n",
       "│    │    └─BasicBlock (0)                    [32, 64, 32, 32]     [32, 128, 16, 16]    (230,144)            False\n",
       "│    │    └─BasicBlock (1)                    [32, 128, 16, 16]    [32, 128, 16, 16]    (295,424)            False\n",
       "│    └─Sequential (6)                         [32, 128, 16, 16]    [32, 256, 8, 8]      --                   False\n",
       "│    │    └─BasicBlock (0)                    [32, 128, 16, 16]    [32, 256, 8, 8]      (919,040)            False\n",
       "│    │    └─BasicBlock (1)                    [32, 256, 8, 8]      [32, 256, 8, 8]      (1,180,672)          False\n",
       "│    └─Sequential (7)                         [32, 256, 8, 8]      [32, 512, 4, 4]      --                   Partial\n",
       "│    │    └─BasicBlock (0)                    [32, 256, 8, 8]      [32, 512, 4, 4]      (3,673,088)          False\n",
       "│    │    └─BasicBlock (1)                    [32, 512, 4, 4]      [32, 512, 4, 4]      4,720,640            Partial\n",
       "│    └─AdaptiveAvgPool2d (8)                  [32, 512, 4, 4]      [32, 512, 1, 1]      --                   --\n",
       "├─Sequential (embedding)                      [32, 512]            [32, 256]            --                   True\n",
       "│    └─Dropout (0)                            [32, 512]            [32, 512]            --                   --\n",
       "│    └─Linear (1)                             [32, 512]            [32, 256]            131,328              True\n",
       "│    └─BatchNorm1d (2)                        [32, 256]            [32, 256]            512                  True\n",
       "=============================================================================================================================\n",
       "Total params: 11,308,352\n",
       "Trainable params: 2,492,672\n",
       "Non-trainable params: 8,815,680\n",
       "Total mult-adds (Units.GIGABYTES): 18.95\n",
       "=============================================================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 415.37\n",
       "Params size (MB): 45.23\n",
       "Estimated Total Size (MB): 466.89\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Resnet18(embedding_dim=256)\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 128, 128),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f03c19",
   "metadata": {},
   "source": [
    "we can save the model state dict using this function. after the model have been trained on the french dataset. We can save the epoch, the optimizer state dict and the scheduler state dict, so we can resume training not start from 0 again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e73d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               scheduler,\n",
    "               target_dir: str,\n",
    "               epoch: int,\n",
    "               model_name: str):\n",
    "    from pathlib import Path\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"Filename must end with .pth or .pt\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }, f=model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca098fc",
   "metadata": {},
   "source": [
    "this function is used to load where we stopped in the last training so it can resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    Loads state_dicts into model, optimizer, scheduler.\n",
    "    Returns the epoch to resume from.\n",
    "    \"\"\"\n",
    "    ckpt = torch.load(checkpoint_path, map_location=next(model.parameters()).device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    return ckpt[\"epoch\"] + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2487e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ckpt_name = f\"ResNet18_TRAINED_ON_FRANCE_DATASET.pth\"\n",
    "ckpt_path = Path(\"models\") / ckpt_name\n",
    "if ckpt_path.exists():\n",
    "    start_epoch = load_checkpoint(ckpt_path, model, optimizer, scheduler)\n",
    "    print(f\"[INFO] Found checkpoint, resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resume training from the last checkpoint\n",
    "# results = train(model=model,\n",
    "#             train_dataloader=egy_train_loader,\n",
    "#             test_dataloader=egy_test_loader,\n",
    "#             optimizer=optimizer,\n",
    "#             loss_fn=loss_fn,\n",
    "#             epochs=10,\n",
    "#             device=device,\n",
    "#             start_epoch=start_epoch,\n",
    "#             scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
